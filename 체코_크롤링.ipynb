{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mí˜„ì¬ ì…€ ë˜ëŠ” ì´ì „ ì…€ì—ì„œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ë™ì•ˆ Kernelì´ ì¶©ëŒí–ˆìŠµë‹ˆë‹¤. \n",
      "\u001b[1;31mì…€ì˜ ì½”ë“œë¥¼ ê²€í† í•˜ì—¬ ê°€ëŠ¥í•œ ì˜¤ë¥˜ ì›ì¸ì„ ì‹ë³„í•˜ì„¸ìš”. \n",
      "\u001b[1;31mìì„¸í•œ ë‚´ìš©ì„ ë³´ë ¤ë©´ <a href='https://aka.ms/vscodeJupyterKernelCrash'>ì—¬ê¸°</a>ë¥¼ í´ë¦­í•˜ì„¸ìš”. \n",
      "\u001b[1;31mìì„¸í•œ ë‚´ìš©ì€ Jupyter <a href='command:jupyter.viewOutput'>ë¡œê·¸</a>ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
     ]
    }
   ],
   "source": [
    "#JadernÃ¡ elektrÃ¡rna Dukovany(ë‘ì½”ë°”ë‹ˆ ì›ìë ¥ ë°œì „ì†Œ)\n",
    "\n",
    "#https://search.seznam.cz/clanky/?q=Jadern%C3%A1%20elektr%C3%A1rna%20Dukovany\n",
    "#ë¦¬ëˆ…ìŠ¤ í™˜ê²½ì—ì„œ ì½”ë“œ ì‹¤í–‰ ì¤‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ê²€ìƒ‰ ì¤‘: ë‘ì½”ë°”ë‹ˆ ì›ì „ (2022.01.01 ~ 2022.03.31)\n",
      "âœ… 198ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! (2022.01.01 ~ 2022.03.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰:  24%|â–ˆâ–ˆâ–       | 47/198 [00:23<00:56,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 1/3): http://dream.kotra.or.kr/kotranews/cms/news/actionKotraBoardDetail.do?SITE_NO=3&MENU_ID=410&CONTENTS_NO=1&bbsGbn=242&bbsSn=242&pNttSn=193341, ì˜¤ë¥˜: HTTPSConnectionPool(host='dream.kotra.or.kr', port=443): Read timed out. (read timeout=10)\n",
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 2/3): http://dream.kotra.or.kr/kotranews/cms/news/actionKotraBoardDetail.do?SITE_NO=3&MENU_ID=410&CONTENTS_NO=1&bbsGbn=242&bbsSn=242&pNttSn=193341, ì˜¤ë¥˜: HTTPSConnectionPool(host='dream.kotra.or.kr', port=443): Read timed out. (read timeout=10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰:  24%|â–ˆâ–ˆâ–       | 48/198 [00:55<24:40,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  HTTP ì˜¤ë¥˜: 404\n",
      "âš  HTTP ì˜¤ë¥˜: 404\n",
      "âš  HTTP ì˜¤ë¥˜: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰:  27%|â–ˆâ–ˆâ–‹       | 54/198 [01:08<04:49,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 1/3): https://www.seoul.co.kr/news/newsView.php?id=20220322500209&wlog_tag3=naver, ì˜¤ë¥˜: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰:  29%|â–ˆâ–ˆâ–‰       | 58/198 [01:16<03:24,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  HTTP ì˜¤ë¥˜: 404\n",
      "âš  HTTP ì˜¤ë¥˜: 404\n",
      "âš  HTTP ì˜¤ë¥˜: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/198 [01:37<01:35,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  HTTP ì˜¤ë¥˜: 404\n",
      "âš  HTTP ì˜¤ë¥˜: 404\n",
      "âš  HTTP ì˜¤ë¥˜: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/198 [01:55<01:07,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  HTTP ì˜¤ë¥˜: 404\n",
      "âš  HTTP ì˜¤ë¥˜: 404\n",
      "âš  HTTP ì˜¤ë¥˜: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/198 [02:13<01:09,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  HTTP ì˜¤ë¥˜: 404\n",
      "âš  HTTP ì˜¤ë¥˜: 404\n",
      "âš  HTTP ì˜¤ë¥˜: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/198 [02:33<01:04,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  HTTP ì˜¤ë¥˜: 404\n",
      "âš  HTTP ì˜¤ë¥˜: 404\n",
      "âš  HTTP ì˜¤ë¥˜: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 185/198 [03:24<00:04,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  HTTP ì˜¤ë¥˜: 404\n",
      "âš  HTTP ì˜¤ë¥˜: 404\n",
      "âš  HTTP ì˜¤ë¥˜: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 196/198 [03:43<00:01,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  HTTP ì˜¤ë¥˜: 404\n",
      "âš  HTTP ì˜¤ë¥˜: 404\n",
      "âš  HTTP ì˜¤ë¥˜: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [03:56<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 2022.01.01 ~ 2022.03.31 í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: naver_news_20220101_20220331.json)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from tqdm import tqdm\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# âœ… í¬ë¡¤ë§í•  ê²€ìƒ‰ì–´ (ë³€ê²½ ê°€ëŠ¥)\n",
    "search_query = \"ë‘ì½”ë°”ë‹ˆ ì›ì „\"\n",
    "\n",
    "# âœ… í¬ë¡¤ë§í•  ê¸°ê°„ ì„¤ì • (ê° ì½”ë“œ ì‹¤í–‰ ì‹œ í•´ë‹¹ ê¸°ê°„ë§Œ ë³€ê²½)\n",
    "START_DATE = \"2022.01.01\"\n",
    "END_DATE = \"2022.03.31\"\n",
    "\n",
    "# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL í…œí”Œë¦¿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# âœ… User-Agent ì„¤ì •\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# âœ… Selenium ì˜µì…˜ ì„¤ì •\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# âœ… ChromeDriver ìë™ ì„¤ì¹˜\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ í¬ë¡¤ë§\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"ğŸ” ê²€ìƒ‰ ì¤‘: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # â³ 3ë¶„(180ì´ˆ) ì´ìƒ ì‹¤í–‰ë˜ë©´ ì¢…ë£Œ\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"â³ ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì§€ë¥¼ ìœ„í•´ í¬ë¡¤ë§ ì¤‘ë‹¨\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ëª©ë¡ í¬ë¡¤ë§ ì‹¤íŒ¨): {e}\")\n",
    "\n",
    "    print(f\"âœ… {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"âš  HTTP ì˜¤ë¥˜: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ì¢…ë£Œ\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, {attempt+1}/{retry_count}): {news['link']}, ì˜¤ë¥˜: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# âœ… í¬ë¡¤ë§ ì‹¤í–‰\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# âœ… JSON íŒŒì¼ ì €ì¥\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… {START_DATE} ~ {END_DATE} í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: {output_file})\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ê²€ìƒ‰ ì¤‘: ë‘ì½”ë°”ë‹ˆ ì›ì „ (2022.04.01 ~ 2022.06.31)\n",
      "âœ… 300ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! (2022.04.01 ~ 2022.06.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰:   8%|â–Š         | 24/300 [00:10<01:57,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 1/3): http://www.sentv.co.kr/news/view/623229, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/623229 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 2/3): http://www.sentv.co.kr/news/view/623229, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/623229 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 3/3): http://www.sentv.co.kr/news/view/623229, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/623229 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰:  33%|â–ˆâ–ˆâ–ˆâ–      | 99/300 [00:56<02:10,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 1/3): http://theviewers.co.kr/View.aspx?No=2379232, ì˜¤ë¥˜: HTTPSConnectionPool(host='theviewers.co.kr', port=443): Read timed out. (read timeout=10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 270/300 [02:27<00:12,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  HTTP ì˜¤ë¥˜: 404\n",
      "âš  HTTP ì˜¤ë¥˜: 404\n",
      "âš  HTTP ì˜¤ë¥˜: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [02:52<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 2022.04.01 ~ 2022.06.31 í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: naver_news_20220401_20220631.json)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# âœ… í¬ë¡¤ë§í•  ê²€ìƒ‰ì–´ (ë³€ê²½ ê°€ëŠ¥)\n",
    "search_query = \"ë‘ì½”ë°”ë‹ˆ ì›ì „\"\n",
    "\n",
    "# âœ… í¬ë¡¤ë§í•  ê¸°ê°„ ì„¤ì • (ê° ì½”ë“œ ì‹¤í–‰ ì‹œ í•´ë‹¹ ê¸°ê°„ë§Œ ë³€ê²½)\n",
    "START_DATE = \"2022.04.01\"\n",
    "END_DATE = \"2022.06.31\"\n",
    "\n",
    "# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL í…œí”Œë¦¿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# âœ… User-Agent ì„¤ì •\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# âœ… Selenium ì˜µì…˜ ì„¤ì •\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# âœ… ChromeDriver ìë™ ì„¤ì¹˜\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ í¬ë¡¤ë§\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"ğŸ” ê²€ìƒ‰ ì¤‘: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # â³ 3ë¶„(180ì´ˆ) ì´ìƒ ì‹¤í–‰ë˜ë©´ ì¢…ë£Œ\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"â³ ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì§€ë¥¼ ìœ„í•´ í¬ë¡¤ë§ ì¤‘ë‹¨\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ëª©ë¡ í¬ë¡¤ë§ ì‹¤íŒ¨): {e}\")\n",
    "\n",
    "    print(f\"âœ… {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"âš  HTTP ì˜¤ë¥˜: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ì¢…ë£Œ\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, {attempt+1}/{retry_count}): {news['link']}, ì˜¤ë¥˜: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# âœ… í¬ë¡¤ë§ ì‹¤í–‰\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# âœ… JSON íŒŒì¼ ì €ì¥\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… {START_DATE} ~ {END_DATE} í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ê²€ìƒ‰ ì¤‘: ë‘ì½”ë°”ë‹ˆ ì›ì „ (2022.07.01 ~ 2022.09.31)\n",
      "âœ… 242ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! (2022.07.01 ~ 2022.09.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 242/242 [01:53<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 2022.07.01 ~ 2022.09.31 í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: naver_news_20220701_20220931.json)\n"
     ]
    }
   ],
   "source": [
    "# âœ… í¬ë¡¤ë§í•  ê²€ìƒ‰ì–´ (ë³€ê²½ ê°€ëŠ¥)\n",
    "search_query = \"ë‘ì½”ë°”ë‹ˆ ì›ì „\"\n",
    "\n",
    "# âœ… í¬ë¡¤ë§í•  ê¸°ê°„ ì„¤ì • (ê° ì½”ë“œ ì‹¤í–‰ ì‹œ í•´ë‹¹ ê¸°ê°„ë§Œ ë³€ê²½)\n",
    "START_DATE = \"2022.07.01\"\n",
    "END_DATE = \"2022.09.31\"\n",
    "\n",
    "# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL í…œí”Œë¦¿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# âœ… User-Agent ì„¤ì •\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# âœ… Selenium ì˜µì…˜ ì„¤ì •\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# âœ… ChromeDriver ìë™ ì„¤ì¹˜\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ í¬ë¡¤ë§\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"ğŸ” ê²€ìƒ‰ ì¤‘: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # â³ 3ë¶„(180ì´ˆ) ì´ìƒ ì‹¤í–‰ë˜ë©´ ì¢…ë£Œ\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"â³ ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì§€ë¥¼ ìœ„í•´ í¬ë¡¤ë§ ì¤‘ë‹¨\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ëª©ë¡ í¬ë¡¤ë§ ì‹¤íŒ¨): {e}\")\n",
    "\n",
    "    print(f\"âœ… {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"âš  HTTP ì˜¤ë¥˜: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ì¢…ë£Œ\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, {attempt+1}/{retry_count}): {news['link']}, ì˜¤ë¥˜: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# âœ… í¬ë¡¤ë§ ì‹¤í–‰\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# âœ… JSON íŒŒì¼ ì €ì¥\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… {START_DATE} ~ {END_DATE} í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ê²€ìƒ‰ ì¤‘: ë‘ì½”ë°”ë‹ˆ ì›ì „ (2022.10.01 ~ 2022.12.31)\n",
      "âœ… 150ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! (2022.10.01 ~ 2022.12.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [01:07<00:00,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 2022.10.01 ~ 2022.12.31 í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: naver_news_20221001_20221231.json)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# âœ… í¬ë¡¤ë§í•  ê²€ìƒ‰ì–´ (ë³€ê²½ ê°€ëŠ¥)\n",
    "search_query = \"ë‘ì½”ë°”ë‹ˆ ì›ì „\"\n",
    "\n",
    "# âœ… í¬ë¡¤ë§í•  ê¸°ê°„ ì„¤ì • (ê° ì½”ë“œ ì‹¤í–‰ ì‹œ í•´ë‹¹ ê¸°ê°„ë§Œ ë³€ê²½)\n",
    "START_DATE = \"2022.10.01\"\n",
    "END_DATE = \"2022.12.31\"\n",
    "\n",
    "# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL í…œí”Œë¦¿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# âœ… User-Agent ì„¤ì •\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# âœ… Selenium ì˜µì…˜ ì„¤ì •\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# âœ… ChromeDriver ìë™ ì„¤ì¹˜\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ í¬ë¡¤ë§\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"ğŸ” ê²€ìƒ‰ ì¤‘: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # â³ 3ë¶„(180ì´ˆ) ì´ìƒ ì‹¤í–‰ë˜ë©´ ì¢…ë£Œ\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"â³ ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì§€ë¥¼ ìœ„í•´ í¬ë¡¤ë§ ì¤‘ë‹¨\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ëª©ë¡ í¬ë¡¤ë§ ì‹¤íŒ¨): {e}\")\n",
    "\n",
    "    print(f\"âœ… {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"âš  HTTP ì˜¤ë¥˜: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ì¢…ë£Œ\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, {attempt+1}/{retry_count}): {news['link']}, ì˜¤ë¥˜: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# âœ… í¬ë¡¤ë§ ì‹¤í–‰\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# âœ… JSON íŒŒì¼ ì €ì¥\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… {START_DATE} ~ {END_DATE} í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ê²€ìƒ‰ ì¤‘: ë‘ì½”ë°”ë‹ˆ ì›ì „ (2023.01.01 ~ 2023.03.31)\n",
      "âœ… 144ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! (2023.01.01 ~ 2023.03.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144/144 [00:55<00:00,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 2023.01.01 ~ 2023.03.31 í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: naver_news_20230101_20230331.json)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# âœ… í¬ë¡¤ë§í•  ê²€ìƒ‰ì–´ (ë³€ê²½ ê°€ëŠ¥)\n",
    "search_query = \"ë‘ì½”ë°”ë‹ˆ ì›ì „\"\n",
    "\n",
    "# âœ… í¬ë¡¤ë§í•  ê¸°ê°„ ì„¤ì • (ê° ì½”ë“œ ì‹¤í–‰ ì‹œ í•´ë‹¹ ê¸°ê°„ë§Œ ë³€ê²½)\n",
    "START_DATE = \"2023.01.01\"\n",
    "END_DATE = \"2023.03.31\"\n",
    "\n",
    "# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL í…œí”Œë¦¿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# âœ… User-Agent ì„¤ì •\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# âœ… Selenium ì˜µì…˜ ì„¤ì •\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# âœ… ChromeDriver ìë™ ì„¤ì¹˜\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ í¬ë¡¤ë§\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"ğŸ” ê²€ìƒ‰ ì¤‘: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # â³ 3ë¶„(180ì´ˆ) ì´ìƒ ì‹¤í–‰ë˜ë©´ ì¢…ë£Œ\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"â³ ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì§€ë¥¼ ìœ„í•´ í¬ë¡¤ë§ ì¤‘ë‹¨\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ëª©ë¡ í¬ë¡¤ë§ ì‹¤íŒ¨): {e}\")\n",
    "\n",
    "    print(f\"âœ… {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"âš  HTTP ì˜¤ë¥˜: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ì¢…ë£Œ\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, {attempt+1}/{retry_count}): {news['link']}, ì˜¤ë¥˜: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# âœ… í¬ë¡¤ë§ ì‹¤í–‰\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# âœ… JSON íŒŒì¼ ì €ì¥\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… {START_DATE} ~ {END_DATE} í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ê²€ìƒ‰ ì¤‘: ë‘ì½”ë°”ë‹ˆ ì›ì „ (2023.04.01 ~ 2023.06.31)\n",
      "âœ… 115ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! (2023.04.01 ~ 2023.06.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 115/115 [00:51<00:00,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 2023.04.01 ~ 2023.06.31 í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: naver_news_20230401_20230631.json)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# âœ… í¬ë¡¤ë§í•  ê²€ìƒ‰ì–´ (ë³€ê²½ ê°€ëŠ¥)\n",
    "search_query = \"ë‘ì½”ë°”ë‹ˆ ì›ì „\"\n",
    "\n",
    "# âœ… í¬ë¡¤ë§í•  ê¸°ê°„ ì„¤ì • (ê° ì½”ë“œ ì‹¤í–‰ ì‹œ í•´ë‹¹ ê¸°ê°„ë§Œ ë³€ê²½)\n",
    "START_DATE = \"2023.04.01\"\n",
    "END_DATE = \"2023.06.31\"\n",
    "\n",
    "# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL í…œí”Œë¦¿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# âœ… User-Agent ì„¤ì •\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# âœ… Selenium ì˜µì…˜ ì„¤ì •\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# âœ… ChromeDriver ìë™ ì„¤ì¹˜\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ í¬ë¡¤ë§\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"ğŸ” ê²€ìƒ‰ ì¤‘: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # â³ 3ë¶„(180ì´ˆ) ì´ìƒ ì‹¤í–‰ë˜ë©´ ì¢…ë£Œ\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"â³ ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì§€ë¥¼ ìœ„í•´ í¬ë¡¤ë§ ì¤‘ë‹¨\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ëª©ë¡ í¬ë¡¤ë§ ì‹¤íŒ¨): {e}\")\n",
    "\n",
    "    print(f\"âœ… {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"âš  HTTP ì˜¤ë¥˜: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ì¢…ë£Œ\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, {attempt+1}/{retry_count}): {news['link']}, ì˜¤ë¥˜: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# âœ… í¬ë¡¤ë§ ì‹¤í–‰\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# âœ… JSON íŒŒì¼ ì €ì¥\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… {START_DATE} ~ {END_DATE} í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ê²€ìƒ‰ ì¤‘: ë‘ì½”ë°”ë‹ˆ ì›ì „ (2023.07.01 ~ 2023.09.31)\n",
      "âœ… 162ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! (2023.07.01 ~ 2023.09.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰:  31%|â–ˆâ–ˆâ–ˆâ–      | 51/162 [00:19<00:48,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 1/3): http://www.sentv.co.kr/news/view/666666, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 2/3): http://www.sentv.co.kr/news/view/666666, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 3/3): http://www.sentv.co.kr/news/view/666666, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 64/162 [00:38<00:35,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 1/3): http://www.sentv.co.kr/news/view/666666, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 2/3): http://www.sentv.co.kr/news/view/666666, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 3/3): http://www.sentv.co.kr/news/view/666666, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 73/162 [00:54<00:44,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 1/3): http://www.sentv.co.kr/news/view/666666, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 2/3): http://www.sentv.co.kr/news/view/666666, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 3/3): http://www.sentv.co.kr/news/view/666666, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 81/162 [01:11<00:59,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 1/3): http://www.sentv.co.kr/news/view/666666, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 2/3): http://www.sentv.co.kr/news/view/666666, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 3/3): http://www.sentv.co.kr/news/view/666666, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 94/162 [01:30<00:21,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 1/3): http://www.sentv.co.kr/news/view/666666, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 2/3): http://www.sentv.co.kr/news/view/666666, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 3/3): http://www.sentv.co.kr/news/view/666666, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 153/162 [02:02<00:02,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 1/3): http://www.sentv.co.kr/news/view/666666, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 2/3): http://www.sentv.co.kr/news/view/666666, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, 3/3): http://www.sentv.co.kr/news/view/666666, ì˜¤ë¥˜: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 162/162 [02:21<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 2023.07.01 ~ 2023.09.31 í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: naver_news_20230701_20230931.json)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# âœ… í¬ë¡¤ë§í•  ê²€ìƒ‰ì–´ (ë³€ê²½ ê°€ëŠ¥)\n",
    "search_query = \"ë‘ì½”ë°”ë‹ˆ ì›ì „\"\n",
    "\n",
    "# âœ… í¬ë¡¤ë§í•  ê¸°ê°„ ì„¤ì • (ê° ì½”ë“œ ì‹¤í–‰ ì‹œ í•´ë‹¹ ê¸°ê°„ë§Œ ë³€ê²½)\n",
    "START_DATE = \"2023.07.01\"\n",
    "END_DATE = \"2023.09.31\"\n",
    "\n",
    "# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL í…œí”Œë¦¿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# âœ… User-Agent ì„¤ì •\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# âœ… Selenium ì˜µì…˜ ì„¤ì •\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# âœ… ChromeDriver ìë™ ì„¤ì¹˜\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ í¬ë¡¤ë§\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"ğŸ” ê²€ìƒ‰ ì¤‘: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # â³ 3ë¶„(180ì´ˆ) ì´ìƒ ì‹¤í–‰ë˜ë©´ ì¢…ë£Œ\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"â³ ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì§€ë¥¼ ìœ„í•´ í¬ë¡¤ë§ ì¤‘ë‹¨\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ëª©ë¡ í¬ë¡¤ë§ ì‹¤íŒ¨): {e}\")\n",
    "\n",
    "    print(f\"âœ… {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"âš  HTTP ì˜¤ë¥˜: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ì¢…ë£Œ\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, {attempt+1}/{retry_count}): {news['link']}, ì˜¤ë¥˜: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# âœ… í¬ë¡¤ë§ ì‹¤í–‰\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# âœ… JSON íŒŒì¼ ì €ì¥\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… {START_DATE} ~ {END_DATE} í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ê²€ìƒ‰ ì¤‘: ë‘ì½”ë°”ë‹ˆ ì›ì „ (2023.10.01 ~ 2023.12.31)\n",
      "âœ… 143ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! (2023.10.01 ~ 2023.12.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:55<00:00,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 2023.10.01 ~ 2023.12.31 í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: naver_news_20231001_20231231.json)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# âœ… í¬ë¡¤ë§í•  ê²€ìƒ‰ì–´ (ë³€ê²½ ê°€ëŠ¥)\n",
    "search_query = \"ë‘ì½”ë°”ë‹ˆ ì›ì „\"\n",
    "\n",
    "# âœ… í¬ë¡¤ë§í•  ê¸°ê°„ ì„¤ì • (ê° ì½”ë“œ ì‹¤í–‰ ì‹œ í•´ë‹¹ ê¸°ê°„ë§Œ ë³€ê²½)\n",
    "START_DATE = \"2023.10.01\"\n",
    "END_DATE = \"2023.12.31\"\n",
    "\n",
    "# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL í…œí”Œë¦¿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# âœ… User-Agent ì„¤ì •\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# âœ… Selenium ì˜µì…˜ ì„¤ì •\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# âœ… ChromeDriver ìë™ ì„¤ì¹˜\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ í¬ë¡¤ë§\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"ğŸ” ê²€ìƒ‰ ì¤‘: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # â³ 3ë¶„(180ì´ˆ) ì´ìƒ ì‹¤í–‰ë˜ë©´ ì¢…ë£Œ\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"â³ ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì§€ë¥¼ ìœ„í•´ í¬ë¡¤ë§ ì¤‘ë‹¨\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ëª©ë¡ í¬ë¡¤ë§ ì‹¤íŒ¨): {e}\")\n",
    "\n",
    "    print(f\"âœ… {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"âš  HTTP ì˜¤ë¥˜: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ì¢…ë£Œ\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, {attempt+1}/{retry_count}): {news['link']}, ì˜¤ë¥˜: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# âœ… í¬ë¡¤ë§ ì‹¤í–‰\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# âœ… JSON íŒŒì¼ ì €ì¥\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… {START_DATE} ~ {END_DATE} í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ê²€ìƒ‰ ì¤‘: ë‘ì½”ë°”ë‹ˆ ì›ì „ (2024.01.01 ~ 2024.03.31)\n",
      "âœ… 167ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! (2024.01.01 ~ 2024.03.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 167/167 [01:26<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 2024.01.01 ~ 2024.03.31 í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: naver_news_20240101_20240331.json)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# âœ… í¬ë¡¤ë§í•  ê²€ìƒ‰ì–´ (ë³€ê²½ ê°€ëŠ¥)\n",
    "search_query = \"ë‘ì½”ë°”ë‹ˆ ì›ì „\"\n",
    "\n",
    "# âœ… í¬ë¡¤ë§í•  ê¸°ê°„ ì„¤ì • (ê° ì½”ë“œ ì‹¤í–‰ ì‹œ í•´ë‹¹ ê¸°ê°„ë§Œ ë³€ê²½)\n",
    "START_DATE = \"2024.01.01\"\n",
    "END_DATE = \"2024.03.31\"\n",
    "\n",
    "# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL í…œí”Œë¦¿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# âœ… User-Agent ì„¤ì •\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# âœ… Selenium ì˜µì…˜ ì„¤ì •\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# âœ… ChromeDriver ìë™ ì„¤ì¹˜\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ í¬ë¡¤ë§\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"ğŸ” ê²€ìƒ‰ ì¤‘: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # â³ 3ë¶„(180ì´ˆ) ì´ìƒ ì‹¤í–‰ë˜ë©´ ì¢…ë£Œ\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"â³ ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì§€ë¥¼ ìœ„í•´ í¬ë¡¤ë§ ì¤‘ë‹¨\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ëª©ë¡ í¬ë¡¤ë§ ì‹¤íŒ¨): {e}\")\n",
    "\n",
    "    print(f\"âœ… {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"âš  HTTP ì˜¤ë¥˜: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ì¢…ë£Œ\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, {attempt+1}/{retry_count}): {news['link']}, ì˜¤ë¥˜: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# âœ… í¬ë¡¤ë§ ì‹¤í–‰\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# âœ… JSON íŒŒì¼ ì €ì¥\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… {START_DATE} ~ {END_DATE} í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ê²€ìƒ‰ ì¤‘: ë‘ì½”ë°”ë‹ˆ ì›ì „ (2024.04.01 ~ 2024.06.31)\n"
     ]
    }
   ],
   "source": [
    "# âœ… í¬ë¡¤ë§í•  ê²€ìƒ‰ì–´ (ë³€ê²½ ê°€ëŠ¥)\n",
    "search_query = \"ë‘ì½”ë°”ë‹ˆ ì›ì „\"\n",
    "\n",
    "# âœ… í¬ë¡¤ë§í•  ê¸°ê°„ ì„¤ì • (ê° ì½”ë“œ ì‹¤í–‰ ì‹œ í•´ë‹¹ ê¸°ê°„ë§Œ ë³€ê²½)\n",
    "START_DATE = \"2024.04.01\"\n",
    "END_DATE = \"2024.06.31\"\n",
    "\n",
    "# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL í…œí”Œë¦¿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# âœ… User-Agent ì„¤ì •\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# âœ… Selenium ì˜µì…˜ ì„¤ì •\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# âœ… ChromeDriver ìë™ ì„¤ì¹˜\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ í¬ë¡¤ë§\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"ğŸ” ê²€ìƒ‰ ì¤‘: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # â³ 3ë¶„(180ì´ˆ) ì´ìƒ ì‹¤í–‰ë˜ë©´ ì¢…ë£Œ\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"â³ ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì§€ë¥¼ ìœ„í•´ í¬ë¡¤ë§ ì¤‘ë‹¨\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ëª©ë¡ í¬ë¡¤ë§ ì‹¤íŒ¨): {e}\")\n",
    "\n",
    "    print(f\"âœ… {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"âš  HTTP ì˜¤ë¥˜: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ì¢…ë£Œ\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, {attempt+1}/{retry_count}): {news['link']}, ì˜¤ë¥˜: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# âœ… í¬ë¡¤ë§ ì‹¤í–‰\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# âœ… JSON íŒŒì¼ ì €ì¥\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… {START_DATE} ~ {END_DATE} í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… í¬ë¡¤ë§í•  ê²€ìƒ‰ì–´ (ë³€ê²½ ê°€ëŠ¥)\n",
    "search_query = \"ë‘ì½”ë°”ë‹ˆ ì›ì „\"\n",
    "\n",
    "# âœ… í¬ë¡¤ë§í•  ê¸°ê°„ ì„¤ì • (ê° ì½”ë“œ ì‹¤í–‰ ì‹œ í•´ë‹¹ ê¸°ê°„ë§Œ ë³€ê²½)\n",
    "START_DATE = \"2024.07.01\"\n",
    "END_DATE = \"2024.09.31\"\n",
    "\n",
    "# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL í…œí”Œë¦¿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# âœ… User-Agent ì„¤ì •\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# âœ… Selenium ì˜µì…˜ ì„¤ì •\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# âœ… ChromeDriver ìë™ ì„¤ì¹˜\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ í¬ë¡¤ë§\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"ğŸ” ê²€ìƒ‰ ì¤‘: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # â³ 3ë¶„(180ì´ˆ) ì´ìƒ ì‹¤í–‰ë˜ë©´ ì¢…ë£Œ\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"â³ ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì§€ë¥¼ ìœ„í•´ í¬ë¡¤ë§ ì¤‘ë‹¨\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ëª©ë¡ í¬ë¡¤ë§ ì‹¤íŒ¨): {e}\")\n",
    "\n",
    "    print(f\"âœ… {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"âš  HTTP ì˜¤ë¥˜: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ì¢…ë£Œ\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, {attempt+1}/{retry_count}): {news['link']}, ì˜¤ë¥˜: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# âœ… í¬ë¡¤ë§ ì‹¤í–‰\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# âœ… JSON íŒŒì¼ ì €ì¥\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… {START_DATE} ~ {END_DATE} í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… í¬ë¡¤ë§í•  ê²€ìƒ‰ì–´ (ë³€ê²½ ê°€ëŠ¥)\n",
    "search_query = \"ë‘ì½”ë°”ë‹ˆ ì›ì „\"\n",
    "\n",
    "# âœ… í¬ë¡¤ë§í•  ê¸°ê°„ ì„¤ì • (ê° ì½”ë“œ ì‹¤í–‰ ì‹œ í•´ë‹¹ ê¸°ê°„ë§Œ ë³€ê²½)\n",
    "START_DATE = \"2024.10.01\"\n",
    "END_DATE = \"2024.12.31\"\n",
    "\n",
    "# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL í…œí”Œë¦¿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# âœ… User-Agent ì„¤ì •\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# âœ… Selenium ì˜µì…˜ ì„¤ì •\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# âœ… ChromeDriver ìë™ ì„¤ì¹˜\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ í¬ë¡¤ë§\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"ğŸ” ê²€ìƒ‰ ì¤‘: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # â³ 3ë¶„(180ì´ˆ) ì´ìƒ ì‹¤í–‰ë˜ë©´ ì¢…ë£Œ\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"â³ ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì§€ë¥¼ ìœ„í•´ í¬ë¡¤ë§ ì¤‘ë‹¨\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ëª©ë¡ í¬ë¡¤ë§ ì‹¤íŒ¨): {e}\")\n",
    "\n",
    "    print(f\"âœ… {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì§„í–‰\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"âš  HTTP ì˜¤ë¥˜: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ì¢…ë£Œ\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš  ì˜¤ë¥˜ ë°œìƒ (ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨, {attempt+1}/{retry_count}): {news['link']}, ì˜¤ë¥˜: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# âœ… í¬ë¡¤ë§ ì‹¤í–‰\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# âœ… JSON íŒŒì¼ ì €ì¥\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… {START_DATE} ~ {END_DATE} í¬ë¡¤ë§ ì™„ë£Œ! (ì €ì¥ íŒŒì¼: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# âœ… JSON íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (naver_news_20240905_20241231.json ì œì™¸)\n",
    "json_files = [f for f in os.listdir() if f.startswith(\"naver_news_\") and f.endswith(\".json\") and f != \"naver_news_20240905_20241231.json\"]\n",
    "\n",
    "# âœ… ëª¨ë“  JSON íŒŒì¼ í•©ì¹˜ê¸°\n",
    "all_news = []\n",
    "for file in json_files:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        news_data = json.load(f)\n",
    "        all_news.extend(news_data)\n",
    "\n",
    "# âœ… í†µí•© JSON íŒŒì¼ ì €ì¥\n",
    "output_file = \"naver_news_all.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_news, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… {len(all_news)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ í†µí•© ì €ì¥í–ˆìŠµë‹ˆë‹¤! (íŒŒì¼ëª…: {output_file})\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
