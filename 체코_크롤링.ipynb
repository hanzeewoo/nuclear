{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IoiGhjuZzD9m",
    "outputId": "a7128646-16ed-4c56-f6ad-5a3e6eb0221d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'apt'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\hansh\\anaconda3\\lib\\site-packages (4.28.1)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\hansh\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hansh\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from selenium) (0.28.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from selenium) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from webdriver-manager) (24.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (25.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\hansh\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!apt update -y\n",
    "!apt install -y chromium-chromedriver\n",
    "!pip install selenium webdriver-manager beautifulsoup4\n",
    "#test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3WEiivfB48Lm",
    "outputId": "42e96234-2477-4cc2-eb81-7c5e15d4efaf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'apt-get'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'apt-get'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'rm'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!apt-get remove -y chromium-browser\n",
    "!apt-get purge -y chromium-chromedriver\n",
    "!apt-get autoremove -y\n",
    "!rm -rf /usr/bin/chromedriver\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gQKH4J9l49--",
    "outputId": "caeca7fc-aad2-4b01-c80e-6d85ac83b13c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'apt'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'wget'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'dpkg'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'apt'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!apt update\n",
    "!apt install -y wget unzip\n",
    "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
    "!dpkg -i google-chrome-stable_current_amd64.deb\n",
    "!apt --fix-broken install -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08LFozSZ5CtV",
    "outputId": "9cf17a2d-4317-4728-970e-a995b97d7a61"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'google-chrome'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!google-chrome --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TseL_vzi5spH",
    "outputId": "1e6c3a65-414e-48bb-9448-bd3be52be215"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'unzip'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'mv'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'chmod'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "CHROME_VERSION = \"133\"  # 위에서 확인한 Chrome 버전 앞 3자리 입력 (예: 120)\n",
    "\n",
    "!wget https://chromedriver.storage.googleapis.com/{CHROME_VERSION}.0.6099.71/chromedriver_linux64.zip\n",
    "!unzip chromedriver_linux64.zip\n",
    "!mv chromedriver /usr/bin/chromedriver\n",
    "!chmod +x /usr/bin/chromedriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pEiScjbn6aUY",
    "outputId": "f9228252-8434-4026-ffd4-76cd18378e21"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'google-chrome'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] 지정된 파일을 찾을 수 없습니다",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msubprocess\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m chrome_version \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mrun([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle-chrome\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--version\u001b[39m\u001b[38;5;124m\"\u001b[39m], capture_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mstdout\n\u001b[0;32m      9\u001b[0m match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m\"\u001b[39m, chrome_version)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m match:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0;32m   1027\u001b[0m                         pass_fds, cwd, env,\n\u001b[0;32m   1028\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[0;32m   1029\u001b[0m                         p2cread, p2cwrite,\n\u001b[0;32m   1030\u001b[0m                         c2pread, c2pwrite,\n\u001b[0;32m   1031\u001b[0m                         errread, errwrite,\n\u001b[0;32m   1032\u001b[0m                         restore_signals,\n\u001b[0;32m   1033\u001b[0m                         gid, gids, uid, umask,\n\u001b[0;32m   1034\u001b[0m                         start_new_session, process_group)\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mCreateProcess(executable, args,\n\u001b[0;32m   1539\u001b[0m                              \u001b[38;5;66;03m# no special security\u001b[39;00m\n\u001b[0;32m   1540\u001b[0m                              \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1541\u001b[0m                              \u001b[38;5;28mint\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m close_fds),\n\u001b[0;32m   1542\u001b[0m                              creationflags,\n\u001b[0;32m   1543\u001b[0m                              env,\n\u001b[0;32m   1544\u001b[0m                              cwd,\n\u001b[0;32m   1545\u001b[0m                              startupinfo)\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1554\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1555\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 지정된 파일을 찾을 수 없습니다"
     ]
    }
   ],
   "source": [
    "# 최신 Chrome 버전 확인\n",
    "!google-chrome --version\n",
    "\n",
    "# Chrome 버전 자동 추출\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "chrome_version = subprocess.run([\"google-chrome\", \"--version\"], capture_output=True, text=True).stdout\n",
    "match = re.search(r\"\\d+\\.\\d+\\.\\d+\\.\\d+\", chrome_version)\n",
    "if match:\n",
    "    CHROME_VERSION = match.group(0)\n",
    "else:\n",
    "    raise Exception(\"Chrome 버전을 찾을 수 없습니다.\")\n",
    "\n",
    "print(f\"✅ Chrome Version Detected: {CHROME_VERSION}\")\n",
    "\n",
    "# Chrome 버전에 맞는 Chromedriver 다운로드\n",
    "CHROMEDRIVER_URL = f\"https://storage.googleapis.com/chrome-for-testing-public/{CHROME_VERSION}/linux64/chromedriver-linux64.zip\"\n",
    "\n",
    "!wget -O chromedriver.zip {CHROMEDRIVER_URL}\n",
    "!unzip chromedriver.zip\n",
    "!mv chromedriver-linux64/chromedriver /usr/bin/chromedriver\n",
    "!chmod +x /usr/bin/chromedriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chrome WebDriver가 정상적으로 실행되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Chrome WebDriver 자동 설치\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "print(\"✅ Chrome WebDriver가 정상적으로 실행되었습니다.\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mh1V8EEX6kjg",
    "outputId": "b88b7a01-9399-4a2e-fdc6-48652a164846"
   },
   "outputs": [],
   "source": [
    "!which chromedriver\n",
    "!chromedriver --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jaderná elektrárna Dukovany(두코바니 원자력 발전소)\n",
    "\n",
    "#https://search.seznam.cz/clanky/?q=Jadern%C3%A1%20elektr%C3%A1rna%20Dukovany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 🔍 검색어 리스트\n",
    "search_queries = [\"두코바니 원전\"]  # 원하는 검색어 입력\n",
    "\n",
    "# ✅ 네이버 뉴스 검색 URL 템플릿\n",
    "NAVER_NEWS_URL_TEMPLATE = \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds=2022.01.01&de=2024.12.31\"\n",
    "\n",
    "# ✅ 크롤링할 날짜 범위\n",
    "START_DATE = datetime.datetime(2024, 9, 5)\n",
    "END_DATE = datetime.datetime(2024, 12, 31)\n",
    "\n",
    "# ✅ User-Agent 설정\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# ✅ Selenium 옵션 설정 (Chrome Headless 모드)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # GUI 없이 실행\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# ✅ ChromeDriver 자동 설치\n",
    "service = Service(ChromeDriverManager().install())  # Chromedriver 자동 다운로드 및 경로 설정\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query):\n",
    "    \"\"\"Selenium을 사용하여 네이버 뉴스 검색 페이지 크롤링 (무한 스크롤)\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "    # ✅ 무한 스크롤을 통해 뉴스 목록 로드\n",
    "    print(f\"🔍 검색 중: {query} (스크롤 크롤링 시작)\")\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(2)  # 로딩 대기\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break  # 더 이상 로딩할 내용 없음\n",
    "        last_height = new_height\n",
    "\n",
    "    # ✅ BeautifulSoup으로 페이지 파싱\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            # ✅ 날짜 가져오기 (목록에서 직접 크롤링)\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ 오류 발생 (목록 크롤링 실패): {e}\")\n",
    "\n",
    "    print(f\"✅ {len(news_list)}개의 뉴스 기사 링크 및 날짜를 수집 완료!\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"뉴스 기사 본문 크롤링\"\"\"\n",
    "    news_data = []\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"📰 뉴스 본문 크롤링 진행\"):\n",
    "        try:\n",
    "            response = requests.get(news[\"link\"], headers=HEADERS)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # ✅ 본문 크롤링 (다양한 선택자 추가)\n",
    "            content_selectors = [\n",
    "                \"div#newsct_article\",  # 최신 네이버 뉴스 본문\n",
    "                \"div#articleBodyContents\",  # 과거 네이버 뉴스 본문\n",
    "                \"div.article_view\",\n",
    "                \"div.news_end\",\n",
    "                \"div#articeBody\",\n",
    "                \"div.content_area\",\n",
    "                \"div#newsEndContents\",\n",
    "                \"article\"\n",
    "            ]\n",
    "            content = \"\"\n",
    "            for selector in content_selectors:\n",
    "                content_element = soup.select(selector)\n",
    "                if content_element:\n",
    "                    content = \" \".join([p.text.strip() for p in content_element])\n",
    "                    break\n",
    "\n",
    "            # ✅ Fallback: Selenium으로 직접 본문 가져오기\n",
    "            if not content.strip():\n",
    "                print(f\"⚠ 본문을 가져오지 못해 Selenium으로 재시도: {news['link']}\")\n",
    "                driver.get(news[\"link\"])\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                content_element = soup.select_one(\"div#newsct_article\") or soup.select_one(\"article\")\n",
    "                if content_element:\n",
    "                    content = \" \".join([p.text.strip() for p in content_element])\n",
    "\n",
    "            # ✅ 기사 본문에서 날짜 확인 (목록과 비교)\n",
    "            meta_date_element = soup.select_one(\"meta[property='article:published_time']\")\n",
    "            if meta_date_element:\n",
    "                article_date = meta_date_element[\"content\"][:10].replace(\"-\", \".\")  # 'YYYY-MM-DD' -> 'YYYY.MM.DD'\n",
    "                news[\"date\"] = article_date  # 목록에서 가져온 날짜를 덮어씀\n",
    "\n",
    "            # ✅ 데이터 저장\n",
    "            news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "            news_data.append(news)\n",
    "\n",
    "            print(f\"✅ 크롤링 완료: {news['title'][:50]}...\")\n",
    "\n",
    "            time.sleep(1)  # 서버 부하 방지\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ 오류 발생 (뉴스 크롤링 실패): {news['link']}, 오류: {e}\")\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# ✅ 전체 검색어에 대해 뉴스 크롤링 실행\n",
    "all_news = []\n",
    "for query in search_queries:\n",
    "    news_list = fetch_naver_news(query)\n",
    "    news_data = fetch_news_content(news_list)\n",
    "    all_news.extend(news_data)\n",
    "\n",
    "# ✅ JSON 파일로 저장\n",
    "output_file = \"naver_news.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_news, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"✅ 총 {len(all_news)}개의 뉴스 기사를 저장했습니다. (파일명: {output_file})\")\n",
    "\n",
    "# ✅ Selenium 종료\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvBil8JomQZ-",
    "outputId": "2d43e6ee-8332-482f-b3df-e60c46473a89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 검색 중: 두코바니 원전 (스크롤 크롤링 시작)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 🔍 검색어 리스트\n",
    "search_queries = [\"두코바니 원전\"]  # 원하는 검색어 입력\n",
    "\n",
    "# ✅ 네이버 뉴스 검색 URL 템플릿\n",
    "NAVER_NEWS_URL_TEMPLATE = \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds=2022.01.01&de=2024.12.31\"\n",
    "\n",
    "# ✅ 크롤링할 날짜 범위\n",
    "START_DATE = datetime.datetime(2022, 1, 1)\n",
    "END_DATE = datetime.datetime(2024, 12, 31)\n",
    "\n",
    "# ✅ User-Agent 설정\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# ✅ Selenium 옵션 설정 (Chrome Headless 모드)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # GUI 없이 실행\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# ✅ ChromeDriver 자동 설치\n",
    "service = Service(ChromeDriverManager().install())  # Chromedriver 자동 다운로드 및 경로 설정\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query):\n",
    "    \"\"\"Selenium을 사용하여 네이버 뉴스 검색 페이지 크롤링 (무한 스크롤)\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "    # ✅ 무한 스크롤을 통해 뉴스 목록 로드\n",
    "    print(f\"🔍 검색 중: {query} (스크롤 크롤링 시작)\")\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(2)  # 로딩 대기\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break  # 더 이상 로딩할 내용 없음\n",
    "        last_height = new_height\n",
    "\n",
    "    # ✅ BeautifulSoup으로 페이지 파싱\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            # ✅ 날짜 가져오기 (목록에서 직접 크롤링)\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ 오류 발생 (목록 크롤링 실패): {e}\")\n",
    "\n",
    "    print(f\"✅ {len(news_list)}개의 뉴스 기사 링크 및 날짜를 수집 완료!\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"뉴스 기사 본문 크롤링\"\"\"\n",
    "    news_data = []\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"📰 뉴스 본문 크롤링 진행\"):\n",
    "        try:\n",
    "            response = requests.get(news[\"link\"], headers=HEADERS)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # ✅ 본문 크롤링 (다양한 선택자 추가)\n",
    "            content_selectors = [\n",
    "                \"div#newsct_article\",  # 최신 네이버 뉴스 본문\n",
    "                \"div#articleBodyContents\",  # 과거 네이버 뉴스 본문\n",
    "                \"div.article_view\",\n",
    "                \"div.news_end\",\n",
    "                \"div#articeBody\",\n",
    "                \"div.content_area\",\n",
    "                \"div#newsEndContents\",\n",
    "                \"article\"\n",
    "            ]\n",
    "            content = \"\"\n",
    "            for selector in content_selectors:\n",
    "                content_element = soup.select(selector)\n",
    "                if content_element:\n",
    "                    content = \" \".join([p.text.strip() for p in content_element])\n",
    "                    break\n",
    "\n",
    "            # ✅ Fallback: Selenium으로 직접 본문 가져오기\n",
    "            if not content.strip():\n",
    "                print(f\"⚠ 본문을 가져오지 못해 Selenium으로 재시도: {news['link']}\")\n",
    "                driver.get(news[\"link\"])\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                content_element = soup.select_one(\"div#newsct_article\") or soup.select_one(\"article\")\n",
    "                if content_element:\n",
    "                    content = \" \".join([p.text.strip() for p in content_element])\n",
    "\n",
    "            # ✅ 기사 본문에서 날짜 확인 (목록과 비교)\n",
    "            meta_date_element = soup.select_one(\"meta[property='article:published_time']\")\n",
    "            if meta_date_element:\n",
    "                article_date = meta_date_element[\"content\"][:10].replace(\"-\", \".\")  # 'YYYY-MM-DD' -> 'YYYY.MM.DD'\n",
    "                news[\"date\"] = article_date  # 목록에서 가져온 날짜를 덮어씀\n",
    "\n",
    "            # ✅ 데이터 저장\n",
    "            news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "            news_data.append(news)\n",
    "\n",
    "            print(f\"✅ 크롤링 완료: {news['title'][:50]}...\")\n",
    "\n",
    "            time.sleep(1)  # 서버 부하 방지\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ 오류 발생 (뉴스 크롤링 실패): {news['link']}, 오류: {e}\")\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# ✅ 전체 검색어에 대해 뉴스 크롤링 실행\n",
    "all_news = []\n",
    "for query in search_queries:\n",
    "    news_list = fetch_naver_news(query)\n",
    "    news_data = fetch_news_content(news_list)\n",
    "    all_news.extend(news_data)\n",
    "\n",
    "# ✅ JSON 파일로 저장\n",
    "output_file = \"naver_news.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_news, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"✅ 총 {len(all_news)}개의 뉴스 기사를 저장했습니다. (파일명: {output_file})\")\n",
    "\n",
    "# ✅ Selenium 종료\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
