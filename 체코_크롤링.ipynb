{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "#Jaderná elektrárna Dukovany(두코바니 원자력 발전소)\n",
    "\n",
    "#https://search.seznam.cz/clanky/?q=Jadern%C3%A1%20elektr%C3%A1rna%20Dukovany\n",
    "#리눅스 환경에서 코드 실행 중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 검색 중: 두코바니 원전 (2022.01.01 ~ 2022.03.31)\n",
      "✅ 198개의 뉴스 기사 수집 완료! (2022.01.01 ~ 2022.03.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행:  24%|██▎       | 47/198 [00:23<00:56,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ 오류 발생 (뉴스 크롤링 실패, 1/3): http://dream.kotra.or.kr/kotranews/cms/news/actionKotraBoardDetail.do?SITE_NO=3&MENU_ID=410&CONTENTS_NO=1&bbsGbn=242&bbsSn=242&pNttSn=193341, 오류: HTTPSConnectionPool(host='dream.kotra.or.kr', port=443): Read timed out. (read timeout=10)\n",
      "⚠ 오류 발생 (뉴스 크롤링 실패, 2/3): http://dream.kotra.or.kr/kotranews/cms/news/actionKotraBoardDetail.do?SITE_NO=3&MENU_ID=410&CONTENTS_NO=1&bbsGbn=242&bbsSn=242&pNttSn=193341, 오류: HTTPSConnectionPool(host='dream.kotra.or.kr', port=443): Read timed out. (read timeout=10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행:  24%|██▍       | 48/198 [00:55<24:40,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ HTTP 오류: 404\n",
      "⚠ HTTP 오류: 404\n",
      "⚠ HTTP 오류: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행:  27%|██▋       | 54/198 [01:08<04:49,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ 오류 발생 (뉴스 크롤링 실패, 1/3): https://www.seoul.co.kr/news/newsView.php?id=20220322500209&wlog_tag3=naver, 오류: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행:  29%|██▉       | 58/198 [01:16<03:24,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ HTTP 오류: 404\n",
      "⚠ HTTP 오류: 404\n",
      "⚠ HTTP 오류: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행:  34%|███▍      | 68/198 [01:37<01:35,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ HTTP 오류: 404\n",
      "⚠ HTTP 오류: 404\n",
      "⚠ HTTP 오류: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행:  39%|███▉      | 78/198 [01:55<01:07,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ HTTP 오류: 404\n",
      "⚠ HTTP 오류: 404\n",
      "⚠ HTTP 오류: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행:  44%|████▍     | 88/198 [02:13<01:09,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ HTTP 오류: 404\n",
      "⚠ HTTP 오류: 404\n",
      "⚠ HTTP 오류: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행:  49%|████▉     | 98/198 [02:33<01:04,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ HTTP 오류: 404\n",
      "⚠ HTTP 오류: 404\n",
      "⚠ HTTP 오류: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행:  93%|█████████▎| 185/198 [03:24<00:04,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ HTTP 오류: 404\n",
      "⚠ HTTP 오류: 404\n",
      "⚠ HTTP 오류: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행:  99%|█████████▉| 196/198 [03:43<00:01,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ HTTP 오류: 404\n",
      "⚠ HTTP 오류: 404\n",
      "⚠ HTTP 오류: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행: 100%|██████████| 198/198 [03:56<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2022.01.01 ~ 2022.03.31 크롤링 완료! (저장 파일: naver_news_20220101_20220331.json)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from tqdm import tqdm\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ✅ 크롤링할 검색어 (변경 가능)\n",
    "search_query = \"두코바니 원전\"\n",
    "\n",
    "# ✅ 크롤링할 기간 설정 (각 코드 실행 시 해당 기간만 변경)\n",
    "START_DATE = \"2022.01.01\"\n",
    "END_DATE = \"2022.03.31\"\n",
    "\n",
    "# ✅ 네이버 뉴스 검색 URL 템플릿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# ✅ User-Agent 설정\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# ✅ Selenium 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# ✅ ChromeDriver 자동 설치\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"네이버 뉴스 검색 페이지에서 기사 링크 크롤링\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"🔍 검색 중: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # ⏳ 3분(180초) 이상 실행되면 종료\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"⏳ 무한 스크롤 방지를 위해 크롤링 중단\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ 오류 발생 (목록 크롤링 실패): {e}\")\n",
    "\n",
    "    print(f\"✅ {len(news_list)}개의 뉴스 기사 수집 완료! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"뉴스 기사 본문 크롤링\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"📰 뉴스 본문 크롤링 진행\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"⚠ HTTP 오류: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # 성공하면 재시도 종료\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ 오류 발생 (뉴스 크롤링 실패, {attempt+1}/{retry_count}): {news['link']}, 오류: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# ✅ 크롤링 실행\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# ✅ JSON 파일 저장\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"✅ {START_DATE} ~ {END_DATE} 크롤링 완료! (저장 파일: {output_file})\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 검색 중: 두코바니 원전 (2022.04.01 ~ 2022.06.31)\n",
      "✅ 300개의 뉴스 기사 수집 완료! (2022.04.01 ~ 2022.06.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행:   8%|▊         | 24/300 [00:10<01:57,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ 오류 발생 (뉴스 크롤링 실패, 1/3): http://www.sentv.co.kr/news/view/623229, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/623229 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "⚠ 오류 발생 (뉴스 크롤링 실패, 2/3): http://www.sentv.co.kr/news/view/623229, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/623229 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "⚠ 오류 발생 (뉴스 크롤링 실패, 3/3): http://www.sentv.co.kr/news/view/623229, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/623229 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행:  33%|███▎      | 99/300 [00:56<02:10,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ 오류 발생 (뉴스 크롤링 실패, 1/3): http://theviewers.co.kr/View.aspx?No=2379232, 오류: HTTPSConnectionPool(host='theviewers.co.kr', port=443): Read timed out. (read timeout=10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행:  90%|█████████ | 270/300 [02:27<00:12,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ HTTP 오류: 404\n",
      "⚠ HTTP 오류: 404\n",
      "⚠ HTTP 오류: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행: 100%|██████████| 300/300 [02:52<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2022.04.01 ~ 2022.06.31 크롤링 완료! (저장 파일: naver_news_20220401_20220631.json)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ✅ 크롤링할 검색어 (변경 가능)\n",
    "search_query = \"두코바니 원전\"\n",
    "\n",
    "# ✅ 크롤링할 기간 설정 (각 코드 실행 시 해당 기간만 변경)\n",
    "START_DATE = \"2022.04.01\"\n",
    "END_DATE = \"2022.06.31\"\n",
    "\n",
    "# ✅ 네이버 뉴스 검색 URL 템플릿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# ✅ User-Agent 설정\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# ✅ Selenium 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# ✅ ChromeDriver 자동 설치\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"네이버 뉴스 검색 페이지에서 기사 링크 크롤링\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"🔍 검색 중: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # ⏳ 3분(180초) 이상 실행되면 종료\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"⏳ 무한 스크롤 방지를 위해 크롤링 중단\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ 오류 발생 (목록 크롤링 실패): {e}\")\n",
    "\n",
    "    print(f\"✅ {len(news_list)}개의 뉴스 기사 수집 완료! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"뉴스 기사 본문 크롤링\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"📰 뉴스 본문 크롤링 진행\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"⚠ HTTP 오류: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # 성공하면 재시도 종료\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ 오류 발생 (뉴스 크롤링 실패, {attempt+1}/{retry_count}): {news['link']}, 오류: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# ✅ 크롤링 실행\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# ✅ JSON 파일 저장\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"✅ {START_DATE} ~ {END_DATE} 크롤링 완료! (저장 파일: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 검색 중: 두코바니 원전 (2022.07.01 ~ 2022.09.31)\n",
      "✅ 242개의 뉴스 기사 수집 완료! (2022.07.01 ~ 2022.09.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행: 100%|██████████| 242/242 [01:53<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2022.07.01 ~ 2022.09.31 크롤링 완료! (저장 파일: naver_news_20220701_20220931.json)\n"
     ]
    }
   ],
   "source": [
    "# ✅ 크롤링할 검색어 (변경 가능)\n",
    "search_query = \"두코바니 원전\"\n",
    "\n",
    "# ✅ 크롤링할 기간 설정 (각 코드 실행 시 해당 기간만 변경)\n",
    "START_DATE = \"2022.07.01\"\n",
    "END_DATE = \"2022.09.31\"\n",
    "\n",
    "# ✅ 네이버 뉴스 검색 URL 템플릿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# ✅ User-Agent 설정\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# ✅ Selenium 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# ✅ ChromeDriver 자동 설치\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"네이버 뉴스 검색 페이지에서 기사 링크 크롤링\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"🔍 검색 중: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # ⏳ 3분(180초) 이상 실행되면 종료\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"⏳ 무한 스크롤 방지를 위해 크롤링 중단\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ 오류 발생 (목록 크롤링 실패): {e}\")\n",
    "\n",
    "    print(f\"✅ {len(news_list)}개의 뉴스 기사 수집 완료! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"뉴스 기사 본문 크롤링\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"📰 뉴스 본문 크롤링 진행\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"⚠ HTTP 오류: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # 성공하면 재시도 종료\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ 오류 발생 (뉴스 크롤링 실패, {attempt+1}/{retry_count}): {news['link']}, 오류: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# ✅ 크롤링 실행\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# ✅ JSON 파일 저장\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"✅ {START_DATE} ~ {END_DATE} 크롤링 완료! (저장 파일: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 검색 중: 두코바니 원전 (2022.10.01 ~ 2022.12.31)\n",
      "✅ 150개의 뉴스 기사 수집 완료! (2022.10.01 ~ 2022.12.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행: 100%|██████████| 150/150 [01:07<00:00,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2022.10.01 ~ 2022.12.31 크롤링 완료! (저장 파일: naver_news_20221001_20221231.json)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ✅ 크롤링할 검색어 (변경 가능)\n",
    "search_query = \"두코바니 원전\"\n",
    "\n",
    "# ✅ 크롤링할 기간 설정 (각 코드 실행 시 해당 기간만 변경)\n",
    "START_DATE = \"2022.10.01\"\n",
    "END_DATE = \"2022.12.31\"\n",
    "\n",
    "# ✅ 네이버 뉴스 검색 URL 템플릿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# ✅ User-Agent 설정\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# ✅ Selenium 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# ✅ ChromeDriver 자동 설치\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"네이버 뉴스 검색 페이지에서 기사 링크 크롤링\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"🔍 검색 중: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # ⏳ 3분(180초) 이상 실행되면 종료\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"⏳ 무한 스크롤 방지를 위해 크롤링 중단\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ 오류 발생 (목록 크롤링 실패): {e}\")\n",
    "\n",
    "    print(f\"✅ {len(news_list)}개의 뉴스 기사 수집 완료! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"뉴스 기사 본문 크롤링\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"📰 뉴스 본문 크롤링 진행\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"⚠ HTTP 오류: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # 성공하면 재시도 종료\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ 오류 발생 (뉴스 크롤링 실패, {attempt+1}/{retry_count}): {news['link']}, 오류: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# ✅ 크롤링 실행\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# ✅ JSON 파일 저장\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"✅ {START_DATE} ~ {END_DATE} 크롤링 완료! (저장 파일: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 검색 중: 두코바니 원전 (2023.01.01 ~ 2023.03.31)\n",
      "✅ 144개의 뉴스 기사 수집 완료! (2023.01.01 ~ 2023.03.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행: 100%|██████████| 144/144 [00:55<00:00,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2023.01.01 ~ 2023.03.31 크롤링 완료! (저장 파일: naver_news_20230101_20230331.json)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ✅ 크롤링할 검색어 (변경 가능)\n",
    "search_query = \"두코바니 원전\"\n",
    "\n",
    "# ✅ 크롤링할 기간 설정 (각 코드 실행 시 해당 기간만 변경)\n",
    "START_DATE = \"2023.01.01\"\n",
    "END_DATE = \"2023.03.31\"\n",
    "\n",
    "# ✅ 네이버 뉴스 검색 URL 템플릿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# ✅ User-Agent 설정\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# ✅ Selenium 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# ✅ ChromeDriver 자동 설치\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"네이버 뉴스 검색 페이지에서 기사 링크 크롤링\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"🔍 검색 중: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # ⏳ 3분(180초) 이상 실행되면 종료\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"⏳ 무한 스크롤 방지를 위해 크롤링 중단\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ 오류 발생 (목록 크롤링 실패): {e}\")\n",
    "\n",
    "    print(f\"✅ {len(news_list)}개의 뉴스 기사 수집 완료! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"뉴스 기사 본문 크롤링\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"📰 뉴스 본문 크롤링 진행\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"⚠ HTTP 오류: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # 성공하면 재시도 종료\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ 오류 발생 (뉴스 크롤링 실패, {attempt+1}/{retry_count}): {news['link']}, 오류: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# ✅ 크롤링 실행\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# ✅ JSON 파일 저장\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"✅ {START_DATE} ~ {END_DATE} 크롤링 완료! (저장 파일: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 검색 중: 두코바니 원전 (2023.04.01 ~ 2023.06.31)\n",
      "✅ 115개의 뉴스 기사 수집 완료! (2023.04.01 ~ 2023.06.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행: 100%|██████████| 115/115 [00:51<00:00,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2023.04.01 ~ 2023.06.31 크롤링 완료! (저장 파일: naver_news_20230401_20230631.json)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ✅ 크롤링할 검색어 (변경 가능)\n",
    "search_query = \"두코바니 원전\"\n",
    "\n",
    "# ✅ 크롤링할 기간 설정 (각 코드 실행 시 해당 기간만 변경)\n",
    "START_DATE = \"2023.04.01\"\n",
    "END_DATE = \"2023.06.31\"\n",
    "\n",
    "# ✅ 네이버 뉴스 검색 URL 템플릿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# ✅ User-Agent 설정\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# ✅ Selenium 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# ✅ ChromeDriver 자동 설치\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"네이버 뉴스 검색 페이지에서 기사 링크 크롤링\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"🔍 검색 중: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # ⏳ 3분(180초) 이상 실행되면 종료\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"⏳ 무한 스크롤 방지를 위해 크롤링 중단\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ 오류 발생 (목록 크롤링 실패): {e}\")\n",
    "\n",
    "    print(f\"✅ {len(news_list)}개의 뉴스 기사 수집 완료! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"뉴스 기사 본문 크롤링\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"📰 뉴스 본문 크롤링 진행\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"⚠ HTTP 오류: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # 성공하면 재시도 종료\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ 오류 발생 (뉴스 크롤링 실패, {attempt+1}/{retry_count}): {news['link']}, 오류: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# ✅ 크롤링 실행\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# ✅ JSON 파일 저장\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"✅ {START_DATE} ~ {END_DATE} 크롤링 완료! (저장 파일: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 검색 중: 두코바니 원전 (2023.07.01 ~ 2023.09.31)\n",
      "✅ 162개의 뉴스 기사 수집 완료! (2023.07.01 ~ 2023.09.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행:  31%|███▏      | 51/162 [00:19<00:48,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ 오류 발생 (뉴스 크롤링 실패, 1/3): http://www.sentv.co.kr/news/view/666666, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "⚠ 오류 발생 (뉴스 크롤링 실패, 2/3): http://www.sentv.co.kr/news/view/666666, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "⚠ 오류 발생 (뉴스 크롤링 실패, 3/3): http://www.sentv.co.kr/news/view/666666, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행:  40%|███▉      | 64/162 [00:38<00:35,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ 오류 발생 (뉴스 크롤링 실패, 1/3): http://www.sentv.co.kr/news/view/666666, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "⚠ 오류 발생 (뉴스 크롤링 실패, 2/3): http://www.sentv.co.kr/news/view/666666, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "⚠ 오류 발생 (뉴스 크롤링 실패, 3/3): http://www.sentv.co.kr/news/view/666666, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행:  45%|████▌     | 73/162 [00:54<00:44,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ 오류 발생 (뉴스 크롤링 실패, 1/3): http://www.sentv.co.kr/news/view/666666, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "⚠ 오류 발생 (뉴스 크롤링 실패, 2/3): http://www.sentv.co.kr/news/view/666666, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "⚠ 오류 발생 (뉴스 크롤링 실패, 3/3): http://www.sentv.co.kr/news/view/666666, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행:  50%|█████     | 81/162 [01:11<00:59,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ 오류 발생 (뉴스 크롤링 실패, 1/3): http://www.sentv.co.kr/news/view/666666, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "⚠ 오류 발생 (뉴스 크롤링 실패, 2/3): http://www.sentv.co.kr/news/view/666666, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "⚠ 오류 발생 (뉴스 크롤링 실패, 3/3): http://www.sentv.co.kr/news/view/666666, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행:  58%|█████▊    | 94/162 [01:30<00:21,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ 오류 발생 (뉴스 크롤링 실패, 1/3): http://www.sentv.co.kr/news/view/666666, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "⚠ 오류 발생 (뉴스 크롤링 실패, 2/3): http://www.sentv.co.kr/news/view/666666, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "⚠ 오류 발생 (뉴스 크롤링 실패, 3/3): http://www.sentv.co.kr/news/view/666666, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행:  94%|█████████▍| 153/162 [02:02<00:02,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ 오류 발생 (뉴스 크롤링 실패, 1/3): http://www.sentv.co.kr/news/view/666666, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "⚠ 오류 발생 (뉴스 크롤링 실패, 2/3): http://www.sentv.co.kr/news/view/666666, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "⚠ 오류 발생 (뉴스 크롤링 실패, 3/3): http://www.sentv.co.kr/news/view/666666, 오류: HTTPSConnectionPool(host='www.sentv.co.kr', port=443): Max retries exceeded with url: /news/view/666666 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행: 100%|██████████| 162/162 [02:21<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2023.07.01 ~ 2023.09.31 크롤링 완료! (저장 파일: naver_news_20230701_20230931.json)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ✅ 크롤링할 검색어 (변경 가능)\n",
    "search_query = \"두코바니 원전\"\n",
    "\n",
    "# ✅ 크롤링할 기간 설정 (각 코드 실행 시 해당 기간만 변경)\n",
    "START_DATE = \"2023.07.01\"\n",
    "END_DATE = \"2023.09.31\"\n",
    "\n",
    "# ✅ 네이버 뉴스 검색 URL 템플릿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# ✅ User-Agent 설정\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# ✅ Selenium 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# ✅ ChromeDriver 자동 설치\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"네이버 뉴스 검색 페이지에서 기사 링크 크롤링\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"🔍 검색 중: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # ⏳ 3분(180초) 이상 실행되면 종료\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"⏳ 무한 스크롤 방지를 위해 크롤링 중단\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ 오류 발생 (목록 크롤링 실패): {e}\")\n",
    "\n",
    "    print(f\"✅ {len(news_list)}개의 뉴스 기사 수집 완료! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"뉴스 기사 본문 크롤링\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"📰 뉴스 본문 크롤링 진행\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"⚠ HTTP 오류: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # 성공하면 재시도 종료\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ 오류 발생 (뉴스 크롤링 실패, {attempt+1}/{retry_count}): {news['link']}, 오류: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# ✅ 크롤링 실행\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# ✅ JSON 파일 저장\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"✅ {START_DATE} ~ {END_DATE} 크롤링 완료! (저장 파일: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 검색 중: 두코바니 원전 (2023.10.01 ~ 2023.12.31)\n",
      "✅ 143개의 뉴스 기사 수집 완료! (2023.10.01 ~ 2023.12.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행: 100%|██████████| 143/143 [00:55<00:00,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2023.10.01 ~ 2023.12.31 크롤링 완료! (저장 파일: naver_news_20231001_20231231.json)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ✅ 크롤링할 검색어 (변경 가능)\n",
    "search_query = \"두코바니 원전\"\n",
    "\n",
    "# ✅ 크롤링할 기간 설정 (각 코드 실행 시 해당 기간만 변경)\n",
    "START_DATE = \"2023.10.01\"\n",
    "END_DATE = \"2023.12.31\"\n",
    "\n",
    "# ✅ 네이버 뉴스 검색 URL 템플릿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# ✅ User-Agent 설정\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# ✅ Selenium 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# ✅ ChromeDriver 자동 설치\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"네이버 뉴스 검색 페이지에서 기사 링크 크롤링\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"🔍 검색 중: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # ⏳ 3분(180초) 이상 실행되면 종료\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"⏳ 무한 스크롤 방지를 위해 크롤링 중단\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ 오류 발생 (목록 크롤링 실패): {e}\")\n",
    "\n",
    "    print(f\"✅ {len(news_list)}개의 뉴스 기사 수집 완료! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"뉴스 기사 본문 크롤링\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"📰 뉴스 본문 크롤링 진행\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"⚠ HTTP 오류: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # 성공하면 재시도 종료\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ 오류 발생 (뉴스 크롤링 실패, {attempt+1}/{retry_count}): {news['link']}, 오류: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# ✅ 크롤링 실행\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# ✅ JSON 파일 저장\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"✅ {START_DATE} ~ {END_DATE} 크롤링 완료! (저장 파일: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 검색 중: 두코바니 원전 (2024.01.01 ~ 2024.03.31)\n",
      "✅ 167개의 뉴스 기사 수집 완료! (2024.01.01 ~ 2024.03.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📰 뉴스 본문 크롤링 진행: 100%|██████████| 167/167 [01:26<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2024.01.01 ~ 2024.03.31 크롤링 완료! (저장 파일: naver_news_20240101_20240331.json)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ✅ 크롤링할 검색어 (변경 가능)\n",
    "search_query = \"두코바니 원전\"\n",
    "\n",
    "# ✅ 크롤링할 기간 설정 (각 코드 실행 시 해당 기간만 변경)\n",
    "START_DATE = \"2024.01.01\"\n",
    "END_DATE = \"2024.03.31\"\n",
    "\n",
    "# ✅ 네이버 뉴스 검색 URL 템플릿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# ✅ User-Agent 설정\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# ✅ Selenium 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# ✅ ChromeDriver 자동 설치\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"네이버 뉴스 검색 페이지에서 기사 링크 크롤링\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"🔍 검색 중: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # ⏳ 3분(180초) 이상 실행되면 종료\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"⏳ 무한 스크롤 방지를 위해 크롤링 중단\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ 오류 발생 (목록 크롤링 실패): {e}\")\n",
    "\n",
    "    print(f\"✅ {len(news_list)}개의 뉴스 기사 수집 완료! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"뉴스 기사 본문 크롤링\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"📰 뉴스 본문 크롤링 진행\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"⚠ HTTP 오류: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # 성공하면 재시도 종료\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ 오류 발생 (뉴스 크롤링 실패, {attempt+1}/{retry_count}): {news['link']}, 오류: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# ✅ 크롤링 실행\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# ✅ JSON 파일 저장\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"✅ {START_DATE} ~ {END_DATE} 크롤링 완료! (저장 파일: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 검색 중: 두코바니 원전 (2024.04.01 ~ 2024.06.31)\n"
     ]
    }
   ],
   "source": [
    "# ✅ 크롤링할 검색어 (변경 가능)\n",
    "search_query = \"두코바니 원전\"\n",
    "\n",
    "# ✅ 크롤링할 기간 설정 (각 코드 실행 시 해당 기간만 변경)\n",
    "START_DATE = \"2024.04.01\"\n",
    "END_DATE = \"2024.06.31\"\n",
    "\n",
    "# ✅ 네이버 뉴스 검색 URL 템플릿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# ✅ User-Agent 설정\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# ✅ Selenium 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# ✅ ChromeDriver 자동 설치\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"네이버 뉴스 검색 페이지에서 기사 링크 크롤링\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"🔍 검색 중: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # ⏳ 3분(180초) 이상 실행되면 종료\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"⏳ 무한 스크롤 방지를 위해 크롤링 중단\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ 오류 발생 (목록 크롤링 실패): {e}\")\n",
    "\n",
    "    print(f\"✅ {len(news_list)}개의 뉴스 기사 수집 완료! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"뉴스 기사 본문 크롤링\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"📰 뉴스 본문 크롤링 진행\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"⚠ HTTP 오류: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # 성공하면 재시도 종료\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ 오류 발생 (뉴스 크롤링 실패, {attempt+1}/{retry_count}): {news['link']}, 오류: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# ✅ 크롤링 실행\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# ✅ JSON 파일 저장\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"✅ {START_DATE} ~ {END_DATE} 크롤링 완료! (저장 파일: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 크롤링할 검색어 (변경 가능)\n",
    "search_query = \"두코바니 원전\"\n",
    "\n",
    "# ✅ 크롤링할 기간 설정 (각 코드 실행 시 해당 기간만 변경)\n",
    "START_DATE = \"2024.07.01\"\n",
    "END_DATE = \"2024.09.31\"\n",
    "\n",
    "# ✅ 네이버 뉴스 검색 URL 템플릿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# ✅ User-Agent 설정\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# ✅ Selenium 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# ✅ ChromeDriver 자동 설치\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"네이버 뉴스 검색 페이지에서 기사 링크 크롤링\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"🔍 검색 중: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # ⏳ 3분(180초) 이상 실행되면 종료\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"⏳ 무한 스크롤 방지를 위해 크롤링 중단\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ 오류 발생 (목록 크롤링 실패): {e}\")\n",
    "\n",
    "    print(f\"✅ {len(news_list)}개의 뉴스 기사 수집 완료! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"뉴스 기사 본문 크롤링\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"📰 뉴스 본문 크롤링 진행\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"⚠ HTTP 오류: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # 성공하면 재시도 종료\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ 오류 발생 (뉴스 크롤링 실패, {attempt+1}/{retry_count}): {news['link']}, 오류: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# ✅ 크롤링 실행\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# ✅ JSON 파일 저장\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"✅ {START_DATE} ~ {END_DATE} 크롤링 완료! (저장 파일: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 크롤링할 검색어 (변경 가능)\n",
    "search_query = \"두코바니 원전\"\n",
    "\n",
    "# ✅ 크롤링할 기간 설정 (각 코드 실행 시 해당 기간만 변경)\n",
    "START_DATE = \"2024.10.01\"\n",
    "END_DATE = \"2024.12.31\"\n",
    "\n",
    "# ✅ 네이버 뉴스 검색 URL 템플릿\n",
    "NAVER_NEWS_URL_TEMPLATE = (\n",
    "    \"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=0&photo=0\"\n",
    "    \"&field=0&pd=3&ds={start_date}&de={end_date}\"\n",
    ")\n",
    "\n",
    "# ✅ User-Agent 설정\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# ✅ Selenium 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# ✅ ChromeDriver 자동 설치\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def fetch_naver_news(query, start_date, end_date):\n",
    "    \"\"\"네이버 뉴스 검색 페이지에서 기사 링크 크롤링\"\"\"\n",
    "    search_url = NAVER_NEWS_URL_TEMPLATE.format(query=query, start_date=start_date, end_date=end_date)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    print(f\"🔍 검색 중: {query} ({start_date} ~ {end_date})\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "        # ⏳ 3분(180초) 이상 실행되면 종료\n",
    "        if time.time() - start_time > 180:\n",
    "            print(\"⏳ 무한 스크롤 방지를 위해 크롤링 중단\")\n",
    "            break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li\")\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title_element = article.select_one(\"a.news_tit\")\n",
    "            title = title_element.text.strip() if title_element else \"N/A\"\n",
    "            link = title_element[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "            date_element = article.select_one(\"span.info\")\n",
    "            date_text = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "            news_list.append({\"title\": title, \"link\": link, \"date\": date_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ 오류 발생 (목록 크롤링 실패): {e}\")\n",
    "\n",
    "    print(f\"✅ {len(news_list)}개의 뉴스 기사 수집 완료! ({start_date} ~ {end_date})\")\n",
    "    return news_list\n",
    "\n",
    "def fetch_news_content(news_list):\n",
    "    \"\"\"뉴스 기사 본문 크롤링\"\"\"\n",
    "    news_data = []\n",
    "    retry_count = 3\n",
    "\n",
    "    for news in tqdm(news_list, desc=\"📰 뉴스 본문 크롤링 진행\"):\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(news[\"link\"], headers=HEADERS, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"⚠ HTTP 오류: {response.status_code}\")\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_selectors = [\n",
    "                    \"div#newsct_article\", \"div#articleBodyContents\", \"div.article_view\",\n",
    "                    \"div.news_end\", \"div#articeBody\", \"div.content_area\",\n",
    "                    \"div#newsEndContents\", \"article\"\n",
    "                ]\n",
    "                content = \"\"\n",
    "                for selector in content_selectors:\n",
    "                    content_element = soup.select(selector)\n",
    "                    if content_element:\n",
    "                        content = \" \".join([p.text.strip() for p in content_element])\n",
    "                        break\n",
    "\n",
    "                news[\"content\"] = content.strip() if content else \"N/A\"\n",
    "                news_data.append(news)\n",
    "\n",
    "                break  # 성공하면 재시도 종료\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ 오류 발생 (뉴스 크롤링 실패, {attempt+1}/{retry_count}): {news['link']}, 오류: {e}\")\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# ✅ 크롤링 실행\n",
    "news_list = fetch_naver_news(search_query, START_DATE, END_DATE)\n",
    "news_data = fetch_news_content(news_list)\n",
    "\n",
    "# ✅ JSON 파일 저장\n",
    "output_file = f\"naver_news_{START_DATE.replace('.', '')}_{END_DATE.replace('.', '')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"✅ {START_DATE} ~ {END_DATE} 크롤링 완료! (저장 파일: {output_file})\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# ✅ JSON 파일 리스트 가져오기 (naver_news_20240905_20241231.json 제외)\n",
    "json_files = [f for f in os.listdir() if f.startswith(\"naver_news_\") and f.endswith(\".json\") and f != \"naver_news_20240905_20241231.json\"]\n",
    "\n",
    "# ✅ 모든 JSON 파일 합치기\n",
    "all_news = []\n",
    "for file in json_files:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        news_data = json.load(f)\n",
    "        all_news.extend(news_data)\n",
    "\n",
    "# ✅ 통합 JSON 파일 저장\n",
    "output_file = \"naver_news_all.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_news, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"✅ {len(all_news)}개의 뉴스 기사를 통합 저장했습니다! (파일명: {output_file})\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
